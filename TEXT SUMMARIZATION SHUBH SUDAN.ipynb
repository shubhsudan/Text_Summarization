{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cac5cd7057602458",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TextSummarization_T5\n",
    "\n",
    "September 18, 2023\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>!pip install datasets -q<br />\n",
    "!pip install huggingface-hub -q<br />\n",
    "!pip install rouge-score -q</p>\n",
    "</blockquote>\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 50%\" />\n",
    "<col style=\"width: 50%\" />\n",
    "</colgroup>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "<p>[1]: !pip install transformers -q<br />\n",
    "!pip install keras_nlp -q</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th>[3]: <strong>from datasets import</strong> load_dataset<br />\n",
    "dataset = load_dataset(\"xsum\", split=\"train\")</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 20%\" />\n",
    "<col style=\"width: 20%\" />\n",
    "<col style=\"width: 20%\" />\n",
    "<col style=\"width: 20%\" />\n",
    "<col style=\"width: 20%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th colspan=\"3\"><blockquote>\n",
    "<p>print(dataset)</p>\n",
    "</blockquote></th>\n",
    "<th></th>\n",
    "<th rowspan=\"2\"><blockquote>\n",
    "<p>| 0.00/5.76k [00:00&lt;?, ?B/s]</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<th colspan=\"3\">Downloading builder script:</th>\n",
    "<th><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td>Downloading readme:</td>\n",
    "<td colspan=\"2\">0%|</td>\n",
    "<td colspan=\"2\" rowspan=\"5\"><blockquote>\n",
    "<p>| 0.00/6.24k [00:00&lt;?, ?B/s]<br />\n",
    "| 0/2 [00:00&lt;?, ?it/s]<br />\n",
    "| 0.00/255M [00:00&lt;?, ?B/s]<br />\n",
    "| 0.00/1.00M [00:00&lt;?, ?B/s]<br />\n",
    "| 0/204045 [00:00&lt;?, ? examples/s]</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td colspan=\"2\">Downloading data files:</td>\n",
    "<td>0%|</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td>Downloading data:</td>\n",
    "<td colspan=\"2\"><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td>Downloading data:</td>\n",
    "<td colspan=\"2\"><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td colspan=\"2\">Generating train split:</td>\n",
    "<td>0%|</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td colspan=\"3\">Generating validation split:</td>\n",
    "<td><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>| 0/11332 [00:00&lt;?, ? examples/s]</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td colspan=\"2\">Generating test split:</td>\n",
    "<td>0%|</td>\n",
    "<td colspan=\"2\"><blockquote>\n",
    "<p>| 0/11334 [00:00&lt;?, ? examples/s]</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> Dataset({  \n",
    "> features: \\['document', 'summary', 'id'\\], num_rows: 204045  \n",
    "> })\n",
    "\n",
    "\\[4\\]: print(dataset\\[0\\])\n",
    "\n",
    "> {'document': 'The full cost of damage in Newton Stewart, one of the\n",
    "> areas worst affected, is still being assessed.\\nRepair work is ongoing\n",
    "> in Hawick and many roads in Peeblesshire remain badly affected by\n",
    "> standing water.\\nTrains on the west coast mainline face disruption due\n",
    "> to damage at the Lamington  \n",
    "> Viaduct.\\nMany businesses and householders were affected by flooding\n",
    "> in Newton Stewart after the River Cree overflowed into the\n",
    "> town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the\n",
    "> damage.\\nThe waters breached a  \n",
    "> retaining wall, flooding many commercial properties on Victoria\n",
    "> Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the\n",
    "> Cinnamon Cafe which was badly affected, said she could not fault the\n",
    "> multi-agency response once the flood hit.\\nHowever, she said more\n",
    "> preventative work could have been carried out to ensure the retaining\n",
    "> wall did not fail.\\n\"It is difficult but I do think there is so much\n",
    "> publicity for Dumfries and the Nith - and I totally appreciate that -\n",
    "> but it is almost like we\\\\'re neglected or forgotten,\" she\n",
    "> said.\\n\"That may not be true but it is perhaps my perspective over the\n",
    "> last few days.\\n\"Why were you not ready to help us a bit more when the\n",
    "> warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert\n",
    "> remains in place across the Borders because of the constant\n",
    "> rain.\\nPeebles was badly hit by problems, sparking calls to introduce\n",
    "> more defences in the area.\\nScottish Borders Council has put a list on\n",
    "> its website of the roads worst affected and drivers have been urged\n",
    "> not to ignore closure signs.\\nThe Labour Party\\\\'s deputy Scottish\n",
    "> leader Alex Rowley was in Hawick on Monday to see the situation first\n",
    "> hand.\\nHe said it was important to get the flood protection plan right\n",
    "> but backed calls to speed up the process.\\n\"I was quite taken aback by\n",
    "> the amount of damage that has been\n",
    "\n",
    "2\n",
    "\n",
    "> done,\" he said.\\n\"Obviously it is heart-breaking for people who have\n",
    "> been forced out of their homes and the impact on businesses.\"\\nHe said\n",
    "> it was important that \"immediate steps\" were taken to protect the\n",
    "> areas most vulnerable and a clear timetable put in place for flood\n",
    "> prevention plans.\\nHave you been affected by flooding in Dumfries and\n",
    "> Galloway or the Borders? Tell us about your experience of the\n",
    "> situation and how it was handled. Email us on selkirk.news@bbc.co.uk\n",
    "> or dumfries@bbc.co.uk.', 'summary': 'Clean-up operations are\n",
    "> continuing across the Scottish Borders and Dumfries and Galloway after\n",
    "> flooding caused by Storm Frank.', 'id': '35232142'}\n",
    ">\n",
    "> **Load Custom Dataset From CSV**\n",
    ">\n",
    "> file_dict = { “train” : “train.csv”, “test” : “test.csv” }\n",
    ">\n",
    "> load_dataset( ‘csv’, data_files=file_dict, delimiter=‘,’,\n",
    "> column_names=\\[‘column01’, ‘column02’,‘column03’\\], skiprows=1 )\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>[5]: datasets =\n",
    "dataset.train_test_split(train_size=0.1,test_size=0.1)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th>[6]: print(len(datasets['train']))<br />\n",
    "print(len(datasets['test']))</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> 20404  \n",
    "> 20405\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 50%\" />\n",
    "<col style=\"width: 50%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><p>[7]: MAX_INPUT_LENGTH = 1024</p>\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>MIN_TARGET_LENGTH = 5 MAX_TARGET_LENGTH = 128 BATCH_SIZE = 8<br />\n",
    "LEARNING_RATE = 2e-5<br />\n",
    "MAX_EPOCHS = 1</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th>MODEL_CHECKPOINT = \"t5-small\"</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table></th>\n",
    "<th><blockquote>\n",
    "<p><em># Name of Model</em></p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th colspan=\"3\"><table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th>[8]: <strong>from transformers import</strong> AutoTokenizer<br />\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td><blockquote>\n",
    "<p>Downloading (…)okenizer_config.json:</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>| 0.00/2.32k [00:00&lt;?, ?B/s]</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td><blockquote>\n",
    "<p>Downloading (…)ve/main/spiece.model:</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>| 0.00/792k [00:00&lt;?, ?B/s]</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td><blockquote>\n",
    "<p>Downloading (…)/main/tokenizer.json:</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>| 0.00/1.39M [00:00&lt;?, ?B/s]</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>[9]: <strong>if</strong> MODEL_CHECKPOINT <strong>in</strong>\n",
    "[\"t5-small\", \"t5-base\"]: prefix = \"summarize: \"<br />\n",
    "<strong>else</strong>:<br />\n",
    "prefix = \"\"</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p><strong>def</strong> preprocess_function(examples):</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\\[10\\]: *#Preprocessing*\n",
    "\n",
    "3\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>inputs = [prefix + doc <strong>for</strong> doc <strong>in</strong>\n",
    "examples[\"document\"]]<br />\n",
    "model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH,␣<br />\n",
    "↪truncation=<strong>True</strong>)<br />\n",
    "<em># Setup the tokenizer for targets</em><br />\n",
    "<strong>with</strong> tokenizer.as_target_tokenizer():<br />\n",
    "labels = tokenizer(<br />\n",
    "examples[\"summary\"], max_length=MAX_TARGET_LENGTH,\n",
    "truncation=<strong>True</strong> )</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>model_inputs[\"labels\"] = labels[\"input_ids\"]</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p><strong>return</strong> model_inputs</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th colspan=\"3\">[11]: tokenized_datasets =\n",
    "datasets.map(preprocess_function, batched=<strong>True</strong>)</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td>Map:</td>\n",
    "<td><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>| 0/20404 [00:00&lt;?, ? examples/s]</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> /usr/local/lib/python3.10/dist-  \n",
    "> packages/transformers/tokenization_utils_base.py:3660: UserWarning:  \n",
    "> \\`as_target_tokenizer\\` is deprecated and will be removed in v5 of\n",
    "> Transformers. You can tokenize your labels by using the argument\n",
    "> \\`text_target\\` of the regular \\`\\_\\_call\\_\\_\\` method (either in the\n",
    "> same call as your input texts if you use the same keyword arguments,\n",
    "> or in a separate call.\n",
    ">\n",
    "> warnings.warn(\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th>Map:</th>\n",
    "<th><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></th>\n",
    "<th><blockquote>\n",
    "<p>| 0/20405 [00:00&lt;?, ? examples/s]</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "| \\[12\\]: **from transformers import** TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq |\n",
    "|------------------------------------------------------------------------|\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 25%\" />\n",
    "<col style=\"width: 25%\" />\n",
    "<col style=\"width: 25%\" />\n",
    "<col style=\"width: 25%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th colspan=\"4\"><blockquote>\n",
    "<p>model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td colspan=\"2\">Downloading (…)lve/main/config.json:</td>\n",
    "<td><blockquote>\n",
    "<p>0%|</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>| 0.00/1.21k [00:00&lt;?, ?B/s]</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td>Downloading model.safetensors:</td>\n",
    "<td>0%|</td>\n",
    "<td colspan=\"2\">| 0.00/242M [00:00&lt;?, ?B/s]</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> All PyTorch model weights were used when initializing\n",
    "> TFT5ForConditionalGeneration.\n",
    ">\n",
    "> All the weights of TFT5ForConditionalGeneration were initialized from\n",
    "> the PyTorch model.\n",
    ">\n",
    "> If your task is similar to the task the model of the checkpoint was\n",
    "> trained on, you can already use TFT5ForConditionalGeneration for\n",
    "> predictions without further training.\n",
    "\n",
    "| \\[13\\]: **from transformers import** DataCollatorForSeq2Seq |\n",
    "|-------------------------------------------------------------|\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>data_collator = DataCollatorForSeq2Seq(tokenizer, model=model,␣</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> ↪return_tensors=\"tf\")\n",
    "\n",
    "4\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th>[14]: train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "batch_size=BATCH_SIZE,<br />\n",
    "columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "shuffle=<strong>True</strong>,<br />\n",
    "collate_fn=data_collator,<br />\n",
    ")</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "batch_size=BATCH_SIZE,<br />\n",
    "columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "shuffle=<strong>False</strong>,<br />\n",
    "collate_fn=data_collator,<br />\n",
    ")</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>generation_dataset = (<br />\n",
    "tokenized_datasets[\"test\"].shuffle().select(list(range(200))).to_tf_dataset(\n",
    "batch_size=BATCH_SIZE,<br />\n",
    "columns=[\"input_ids\", \"attention_mask\", \"labels\"],<br />\n",
    "shuffle=<strong>False</strong>,<br />\n",
    "collate_fn=data_collator,<br />\n",
    ")<br />\n",
    ")</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> You're using a T5TokenizerFast tokenizer. Please note that with a\n",
    "> fast  \n",
    "> tokenizer, using the \\`\\_\\_call\\_\\_\\` method is faster than using a\n",
    "> method to encode the text followed by a call to the \\`pad\\` method to\n",
    "> get a padded encoding.\n",
    "\n",
    "| \\[15\\]: optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE) model.compile(optimizer=optimizer) |\n",
    "|------------------------------------------------------------------------|\n",
    "\n",
    "\\[16\\]: model.summary()\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "<col style=\"width: 33%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th colspan=\"3\">Model: \"tft5_for_conditional_generation\"<br />\n",
    "_________________________________________________________________</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td><blockquote>\n",
    "<p>Layer (type)</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>Output Shape</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>Param #</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td\n",
    "colspan=\"3\">=================================================================</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td><blockquote>\n",
    "<p>shared (Embedding)</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>multiple</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>16449536</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td><blockquote>\n",
    "<p>encoder (TFT5MainLayer)</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>multiple</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>35330816</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td><blockquote>\n",
    "<p>decoder (TFT5MainLayer)</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>multiple</p>\n",
    "</blockquote></td>\n",
    "<td><blockquote>\n",
    "<p>41625344</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> =================================================================\n",
    "> Total params: 60506624 (230.81 MB)  \n",
    "> Trainable params: 60506624 (230.81 MB)  \n",
    "> Non-trainable params: 0 (0.00 Byte)  \n",
    "> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "\n",
    "5\n",
    "\n",
    "\\[17\\]: **import keras_nlp**\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 50%\" />\n",
    "<col style=\"width: 50%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th colspan=\"2\"><blockquote>\n",
    "<p>rouge_l = keras_nlp.metrics.RougeL()</p>\n",
    "<p><strong>def</strong> metric_fn(eval_predictions):</p>\n",
    "<p>predictions, labels = eval_predictions</p>\n",
    "<p>decoded_predictions = tokenizer.batch_decode(predictions,␣</p>\n",
    "<p>↪skip_special_tokens=<strong>True</strong>)</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td><blockquote>\n",
    "<p>label[label &lt; 0] = tokenizer.pad_token_id</p>\n",
    "</blockquote></td>\n",
    "<td><em># Replace masked label tokens</em></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td colspan=\"2\"><blockquote>\n",
    "<p>decoded_labels = tokenizer.batch_decode(labels,\n",
    "skip_special_tokens=<strong>True</strong>) result =\n",
    "rouge_l(decoded_labels, decoded_predictions)<br />\n",
    "<em># We will print only the F1 score, you can use other aggregation\n",
    "metrics as</em>␣↪<em>well</em> result = {\"RougeL\":\n",
    "result[\"f1_score\"]}</p>\n",
    "</blockquote></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p><strong>return</strong> result</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> Using TensorFlow backend\n",
    "\n",
    "| \\[18\\]: **from transformers.keras_callbacks import** KerasMetricCallback |\n",
    "|------------------------------------------------------------------------|\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>metric_callback = KerasMetricCallback(metric_fn,␣<br />\n",
    "↪eval_dataset=generation_dataset,\n",
    "predict_with_generate=<strong>True</strong>)</p>\n",
    "<p>callbacks = [metric_callback]</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p><em># For now we will use our test set as our\n",
    "validation_data</em><br />\n",
    "model.fit(train_dataset, validation_data=test_dataset,\n",
    "epochs=MAX_EPOCHS,␣</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> ↪callbacks=callbacks)  \n",
    "> WARNING:root:No label_cols specified for KerasMetricCallback, assuming\n",
    "> you want the 'labels' key.\n",
    ">\n",
    "> 2551/2551 \\[==============================\\] - ETA: 0s - loss: 2.9181\n",
    ">\n",
    "> /usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:834:\n",
    "> UserWarning: Using the model-agnostic default \\`max_length\\` (=20) to\n",
    "> control the generation length. recommend setting \\`max_new_tokens\\` to\n",
    "> control the maximum length of the generation.\n",
    ">\n",
    "> warnings.warn(\n",
    ">\n",
    "> 2551/2551 \\[==============================\\] - 896s 337ms/step - loss:\n",
    "> 2.9181 -val_loss: 2.5862 - RougeL: 0.2018\n",
    "\n",
    "\\[18\\]: \\<keras.src.callbacks.History at 0x7db2c3e7bf70\\>\n",
    "\n",
    "6\n",
    "\n",
    "\\[21\\]: **from transformers import** pipeline\n",
    "\n",
    "<table>\n",
    "<colgroup>\n",
    "<col style=\"width: 100%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><blockquote>\n",
    "<p>summarizer = pipeline(\"summarization\", model=model,\n",
    "tokenizer=tokenizer,␣ ↪framework=\"tf\")<br />\n",
    "print(\"________Original Article__________\")<br />\n",
    "print(datasets['test'][0]['document'])<br />\n",
    "print(\"-\" * 1000)<br />\n",
    "print(\"________Original Summary__________\")<br />\n",
    "print(datasets['test'][0]['summary'])<br />\n",
    "print(\"-\" * 1000)<br />\n",
    "print(\"________Predicted Summary__________\")</p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> summarizer(datasets\\[\"test\"\\]\\[0\\]\\[\"document\"\\],min_length=MIN_TARGET_LENGTH,max_length=MAX_TARGET\n",
    ">\n",
    "> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
    "> \\_\\_\\_\\_\\_\\_\\_\\_Original Article\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_  \n",
    "> Richard Lochhead, who will be in the US this week, said selling\n",
    "> Scottish haggis to the Americans would be worth millions to the\n",
    "> Scottish economy.\n",
    ">\n",
    "> Haggis imports have been outlawed in the US since 1971.\n",
    ">\n",
    "> The country's food standards agency prohibits sheep lungs - one of the\n",
    "> key ingredients - in food products.\n",
    ">\n",
    "> Mr Lochhead flew to Canada on Sunday and will travel on to the US\n",
    "> later. During his time in Washington DC, he will try to persuade the\n",
    "> US government to allow Scottish producers to import haggis to the\n",
    "> country - the latest in a series of attempts by the Scottish and UK\n",
    "> governments to do so.\n",
    ">\n",
    "> The rural affairs secretary told the BBC: \"Tens of millions of\n",
    "> Americans want to enjoy Scotland's national dish. Now it may be that\n",
    "> we'd have to tweak the recipe for haggis to get into the US market,\n",
    "> because some of the ingredients - such as sheep lungs - have been\n",
    "> banned since 1971.\n",
    ">\n",
    "> \"But I think our own producers here in Scotland are up for tweaking\n",
    "> the recipe so that US customers can still get as close as possible to\n",
    "> the real thing. \"And if we managed to get into that market that would\n",
    "> create jobs back here in Scotland and millions of pounds to the\n",
    "> Scottish economy.\"  \n",
    "> Tweaking the haggis  \n",
    "> Two leading Scottish butchers have said they are happy to make changes\n",
    "> to their\n",
    "\n",
    "7\n",
    "\n",
    "> traditional Scottish recipes to make it suitable for the US market.\n",
    ">\n",
    "> They are looking into adapted haggis recipes which do not include\n",
    "> ingredients -such as sheep lung - which are not allowed across the\n",
    "> pond.\n",
    ">\n",
    "> Read more about their plans.\n",
    ">\n",
    "> The US ban on imports was raised in June 2014 by the UK Environment\n",
    "> Secretary Owen Paterson who spoke to senior officials from the Obama\n",
    "> administration after speaking to producers at the Royal Highland Show\n",
    "> in Edinburgh.\n",
    ">\n",
    "> Mr Lochhead has also made a previous attempt to persuade Americans to\n",
    "> accept the haggis, inviting a delegation from the US to come to\n",
    "> Scotland in 2011 in a bid to overturn the ban.\n",
    ">\n",
    "> And in 2005, President George W Bush was even lobbied directly on\n",
    "> haggis when he was at the G8 summit in Gleneagles.\n",
    ">\n",
    "> But so far the US Food and Drug Administration has refused to reverse\n",
    "> the decision.\n",
    ">\n",
    "> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
    "> \\_\\_\\_\\_\\_\\_\\_\\_Original Summary\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_  \n",
    "> The recipe for haggis should be \"tweaked\" to get round a decades-old\n",
    "> ban on the food in the US, Scotland's rural affairs secretary has\n",
    "> said.\n",
    ">\n",
    "> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
    "> \\_\\_\\_\\_\\_\\_\\_\\_Predicted Summary\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "\n",
    "\\[21\\]: \\[{'summary_text': 'Scottish haggis imports have been outlawed\n",
    "in the US since 1971, butchers have said.'}\\]\n",
    "\n",
    "8\n",
    "\n",
    "| \\[ \\]: model.save_pretrained(\"T5CustomSummarizer\",from_pt=**True**) tokenizer.save_pretrained(\"T5CustomSumTokenizer\",from_pt=**True**) |\n",
    "|------------------------------------------------------------------------|\n",
    "\n",
    "9 #GITHUB LINK\n",
    "#https://github.com/shubhsudan/Text_Summarization.git\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
