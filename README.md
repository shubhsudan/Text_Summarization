# Text_Summarization


This repository contains the code and instructions for text summarization using the T5 model with a focus on optimizing model parameters, Adam optimizer hyperparameters, batch size, and epochs. The goal is to achieve a minimum RougeL score of 0.35 while producing high-quality summaries. The assignment will be assessed according to a rubric, and results will be compared with other approaches.

Table of Contents

Dataset
Getting Started
T5 Model Optimization
Adam Optimizer Hyperparameters
Batch Size and Epochs Optimization
RougeL Score
Results
Contributors
License
Acknowledgments
Contact
Dataset

XSum Dataset
The XSum dataset is used for this text summarization project. It includes document-summary pairs. You can obtain the dataset here. Ensure that the dataset is properly preprocessed before using it in the code.


Adam Optimizer Hyperparameters

Optimizing the Adam optimizer hyperparameters is crucial for training a deep learning model effectively. Describe how to optimize parameters like learning rate, beta1, and beta2. Include any code examples or configurations for this optimization.

Also optimize batch_size and epochs as per accuracy requirement.  

Make sure the RougeL Score is a minimum of 0.35 and summaries should be good enough.

